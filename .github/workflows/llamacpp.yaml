name: llama.cpp

defaults:
  run:
    shell: bash -el {0}
on:
  workflow_dispatch:
  pull_request:
    # paths:
    #   - '.github/workflows/llamacpp.yaml'
    #   - 'requirements/**'
  schedule:
    - cron: "0 0 * * *"

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  issues: write

jobs:
    unit-tests:
      if: contains(github.event.pull_request.labels.*.name, 'Ascend NPU')
          
      runs-on: ubuntu-latest
      strategy:
        matrix:
          build: ['Release']
          cann: ['openeuler-python3.10-cann8.0.rc3.beta1']
          device: ['ascend910b3']
      container:
        image: ascendai/cann:${{ matrix.cann }}
      steps:
      - uses: actions/checkout@v4
      - name: Install llamacpp
        uses: nick-fields/retry@v3
        with:
          timeout_minutes: 30
          max_attempts: 3
          retry_on: error
          command: |
            yum update -y
            yum install git cmake gcc gcc-c++ make -y  
            git clone https://github.com/ggerganov/llama.cpp.git

      - name: Build
        run: |
            cd llama.cpp
            mkdir build
            cd build
            export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/lib64:/usr/local/Ascend/ascend-toolkit/latest/x86_64-linux/devlib/:${LD_LIBRARY_PATH}
            cmake .. -DCMAKE_BUILD_TYPE=${{ matrix.build }} -DGGML_CANN=on -DSOC_TYPE=${{ matrix.device }} && cmake --build . -j $(nproc)
