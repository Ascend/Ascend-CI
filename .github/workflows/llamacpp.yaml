name: llama.cpp

defaults:
  run:
    shell: bash -ieo pipefail {0}
on:
  workflow_dispatch:
  pull_request:
    paths:
      - '.github/workflows/llamacpp.yaml'
      - 'requirements/**'
  schedule:
    - cron: "0 0 * * *"
  push:
    paths:
        - '.github/workflows/llamacpp.yaml'

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  issues: write

jobs:
  unit-tests:
    runs-on: [self-hosted, ascend, npu]
    container:
      image: ascendai/cann
      ports:
        - 80
      volumes:
        - /usr/local/dcmi:/usr/local/dcmi
        - /usr/local/bin/npu-smi:/usr/local/bin/npu-smi 
        - /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/
        - /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info
        - /etc/ascend_install.info:/etc/ascend_install.info   
      options: --network host
               --name llamacpp_build
               --device /dev/davinci5
               --device /dev/davinci_manager
               --device /dev/devmm_svm
               --device /dev/hisi_hdc
               --entrypoint /bin/bash
                              
    steps:
    - uses: actions/checkout@v4

    - name: Install llamacpp
      uses: nick-fields/retry@v3
      with:
        timeout_minutes: 30
        max_attempts: 3
        retry_on: error
        command: | 
          npu-smi info
          apt-get update 
          apt-get install cmake make git build-essential -y
          pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
          source /root/.bashrc

          gcc --version
          git clone https://github.com/ggerganov/llama.cpp.git
          cd llama.cpp
          
    - name: Build 
      run: |
          make
          # cmake -B build
          # cmake --build build --config Release
    



    

    
    
