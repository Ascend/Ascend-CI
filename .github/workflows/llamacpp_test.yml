name: Test llamacpp

on:
  workflow_dispatch:
    inputs:
      runner:
        required: true
        type: choice
        options:
          - linux-arm64-npu-1
          - linux-arm64-npu-2
          - linux-arm64-npu-4
          - linux-aarch64-310p-1
          - linux-aarch64-310p-2
        default: "linux-arm64-npu-1"
        description: "The runner selected to run on"
    
  pull_request:
    branches:
      - "main"
    paths:
      - ".github/workflows/llamacpp_test.yml"
  push:
    branches:
      - "main"
    paths:
      - ".github/workflows/llamacpp_test.yml"
  
  schedule:
    - cron: '0 12 * * *'

      
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  prepare:
    runs-on: ubuntu-latest
    outputs:
      runner: ${{ steps.set.outputs.runner }}
      images: ${{ steps.set.outputs.images }}
      device: ${{ steps.set.outputs.device }}
    steps:
      - name: Set outputs
        id: set
        run: |
          runner="${{ inputs.runner || 'linux-arm64-npu-1' }}"
          echo "runner=${runner}" >>"${GITHUB_OUTPUT}"

          if [[ "${runner}" == linux-aarch64-310p-* ]]; then
            image='["swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/cann:8.1.rc1-310p-ubuntu22.04-py3.11"]'
            device='["ascend310p"]'
          else
            image='["swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/cann:8.1.rc1-910b-ubuntu22.04-py3.11"]'
            # image='["8.3.rc2-910-ubuntu22.04-py3.11"]'
            device='["ascend910b"]'
          fi

          {
            echo "images=${image}"
            echo "device=${device}"
          } >>"${GITHUB_OUTPUT}"

  test:
    name: Build llama.cpp on OpenEuler for Arm64
    needs: prepare
    outputs:
      check-changed: ${{ steps.check-changed.outputs.changed }}
      upload-report: ${{ steps.upload-report.outputs.artifact-url }}
    strategy:
      matrix:
        image: ${{ fromJSON(needs.prepare.outputs.images) }}
        device: ${{ fromJSON(needs.prepare.outputs.device) }}
        build: ['Release']
    runs-on: ${{ needs.prepare.outputs.runner }}
    container:
      image: ${{ matrix.image }}
      env:
        HF_ENDPOINT: https://hf-mirror.com

    steps:
      - name: Show NPU info
        run: |
          npu-smi info

      - name: Config mirrors
        run: |
          sed -i 's|ports.ubuntu.com|mirrors.tuna.tsinghua.edu.cn|g' /etc/apt/sources.list
          pip config set global.index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple

      - name: Install system dependencies
        run: |
          pkg_manager=""
          for cmd in apt yum dnf; do
          if command -v $cmd >/dev/null 2>&1; then
              pkg_manager=$cmd
              break
          fi
          done

          if [ "$pkg_manager" = "apt" ]; then
            apt-get update
            apt-get install -y \
              git gcc g++ make cmake ninja-build curl \
              libgl1 libglib2.0-0 libsndfile1 libcurl4-openssl-dev unzip
          elif [ "$pkg_manager" = "yum" ] || [ "$pkg_manager" = "dnf" ]; then
            $pkg_manager install -y \
              git gcc gcc-c++ make cmake libcurl-devel unzip
          else
            echo "Unsupported package manager. Please install dependencies manually."
            exit 1
          fi

      - name: Config git
        run: |
          git config --global --add safe.directory "$GITHUB_WORKSPACE"

      - name: Checkout
        uses: actions/checkout@v4

      - name: Checkout llama.cpp
        uses: actions/checkout@v4
        with:
          repository: ggerganov/llama.cpp
          path: llama.cpp

      - name: Install torch
        run: |
          pip install torch torch_npu==2.7.1rc1 transformers datasets evaluate huggingface_hub mistral_common sentencepiece

      - name: List torch version
        id: list-torch-version
        shell: bash
        run: |
          torch_version=$(python -c "from importlib.metadata import version;print(version('torch'))")
          echo "torch-version=${torch_version}" >> $GITHUB_OUTPUT

      - name: Build llama.cpp
        working-directory: llama.cpp
        run: |
          export LD_LIBRARY_PATH=${ASCEND_TOOLKIT_HOME}/lib64:${ASCEND_TOOLKIT_HOME}/$(uname -m)-linux/devlib/:${LD_LIBRARY_PATH}

          cmake -S . -B build \
            -DCMAKE_BUILD_TYPE=${{ matrix.build }} \
            -DGGML_CANN=on \
            -DSOC_TYPE=${{ matrix.device }}
          cmake --build build -j $(nproc)
    
      - name: Define operator list
        id: define-ops
        run: |
            echo "ops=ABS,ACC,ADD,ADD1,ARANGE,ARGMAX,ARGSORT,CLAMP,CONCAT,CONT,CONV_2D,CONV_2D_DW, \
              CONV_TRANSPOSE_1D,CONV_TRANSPOSE_2D,COS,COUNT_EQUAL,CPY,CROSS_ENTROPY_LOSS,CROSS_ENTROPY_LOSS_BACK, \
              DIAG_MASK_INF,DIV,DUP,ELU,EXP,FLASH_ATTN_EXT,GATED_LINEAR_ATTN,GEGLU,GEGLU_ERF,GEGLU_QUICK, \
              GELU,GELU_ERF,GELU_QUICK,GET_ROWS,GET_ROWS_BACK,GROUP_NORM,HARDSIGMOID,HARDSWISH,IM2COL,L2_NORM, \
              LEAKY_RELU,LOG,MEAN,MUL,MUL_MAT,MUL_MAT_ID,NEG,NORM,OPT_STEP_ADAMW,OUT_PROD,PAD,PAD_REFLECT_1D, \
              POOL_2D,REGLU,RELU,REPEAT,REPEAT_BACK,RMS_NORM,RMS_NORM_BACK,RMS_NORM_MUL_ADD,ROLL,ROPE,ROPE_BACK, \
              RWKV_WKV6,RWKV_WKV7,SCALE,SET,SET_ROWS,SGN,SIGMOID,SILU,SILU_BACK,SIN,SOFTCAP,SOFT_MAX,SOFT_MAX_BACK, \
              SQR,SQRT,SSM_CONV,SSM_SCAN,STEP,SUB,SUM,SUM_ROWS,SWIGLU,TANH,TIMESTEP_EMBEDDING,UPSCALE" >> $GITHUB_OUTPUT

      # - name: test backend ops
      #   id: backend-ops
      #   working-directory: llama.cpp/build/bin
      #   run: |
      #     set -o pipefail
      #     LOG_DIR="backend-op-logs"
      #     mkdir -p "${LOG_DIR}"
      #     failed_ops=""
      #     for op in $(echo ${{ steps.define-ops.outputs.ops }} | tr ',' '\n')
      #     do
      #       echo "Testing operator: $op"
      #       log_file="${LOG_DIR}/${op}.log"
      #       if ! ./test-backend-ops test -o "$op" 2>&1 | tee "${log_file}"; then
      #         echo "Operator $op failed, continuing..."
      #         failed_ops="${failed_ops}${op}\n"
      #       fi
      #     done

      #     if [ -n "$failed_ops" ]; then
      #       echo -e "The following operators failed:\n$failed_ops"
      #     else
      #       echo "All operators passed."
      #     fi
      #     echo "log-dir=${PWD}/${LOG_DIR}" >> "${GITHUB_OUTPUT}"
      #   shell:
      #     bash

      # - name: Summarize backend operator support
      #   working-directory: llama.cpp
      #   run: |
      #     python ../script/classify_backend_ops.py \
      #       --log-dir "${{ steps.backend-ops.outputs.log-dir }}" \
      #       --summary-file ../docs/llamacpp_ops.md

      - name: check if ops changed
        id: check-changed
        run: |
          echo "test" >> docs/llamacpp_ops.md
          cat docs/llamacpp_ops.md
          git diff --exit-code -- docs/llamacpp_ops.md || echo "changed=true" >> $GITHUB_OUTPUT
        shell: bash

      - name: Upload the llama.cpp backend op test logs
        id: upload-report
        uses: actions/upload-artifact@v4
        with:
          name: llamacpp_ops.md
          path: docs/llamacpp_ops.md
          if-no-files-found: error
          retention-days: 1
          overwrite: true
    
      # - name: Download model and dateset
      #   working-directory: llama.cpp
      #   run: |
      #     mkdir -p models/Qwen2.5-0.5B-Instruct
      #     huggingface-cli download \
      #       --resume-download Qwen/Qwen2.5-0.5B-Instruct \
      #       --local-dir models/Qwen2.5-0.5B-Instruct
          
      #     mkdir -p models/Qwen2.5-VL-3B-Instruct
      #     huggingface-cli download \
      #       --resume-download Qwen/Qwen2.5-VL-3B-Instruct \
      #       --local-dir models/Qwen2.5-VL-3B-Instruct
          
      #     mkdir -p data
      #     huggingface-cli download --repo-type dataset\
      #       --resume-download ggml-org/ci \
      #       --local-dir data
          
      #     unzip -o data/wikitext-2-raw-v1.zip -d data/
      #     rm -f data/wikitext-2-raw-v1.zip

      # - name: convert Qwen2.5 model
      #   working-directory: llama.cpp
      #   run: |
      #     python3 convert_hf_to_gguf.py models/Qwen2.5-0.5B-Instruct \
      #       --outfile models/Qwen2.5-0.5B-Instruct/ggml-model-f16.gguf \
      #       --outtype f16

      #     python3 convert_hf_to_gguf.py models/Qwen2.5-VL-3B-Instruct \
      #       --outfile models/Qwen2.5-VL-3B-Instruct/ggml-model-f16.gguf \
      #       --outtype f16

      #     python3 convert_hf_to_gguf.py models/Qwen2.5-VL-3B-Instruct \
      #       --outfile models/Qwen2.5-VL-3B-Instruct/mmproj.gguf \
      #       --outtype f16 \
      #       --mmproj

      # - name: Accuracy check
      #   working-directory: llama.cpp
      #   run: |
      #     ./build/bin/llama-perplexity  \
      #       --model models/Qwen2.5-0.5B-Instruct/ggml-model-f16.gguf \
      #       -f data/wikitext-2-raw/wiki.test.raw \
      #       -t 8 \
      #       -c 4096 \
      #       -ngl 32 \
      #       --batch-size 512 \
      #       --ppl-stride 64 \
      #       --ppl-output-type 1

      # - name: Run llama-bench
      #   working-directory: llama.cpp
      #   run: |
      #     ./build/bin/llama-bench \
      #       --model models/Qwen2.5-0.5B-Instruct/ggml-model-f16.gguf \
      #       --n-prompt 256 \
      #       --n-gen 64 \
      #       --batch-size 512 \
      #       --ubatch-size 128 \
      #       --threads 16 \
      #       --repetitions 3 \
      #       --output json \
      #       --progress

      # - name: Run llama-batched-bench
      #   working-directory: llama.cpp
      #   run: |
      #     ./build/bin/llama-batched-bench \
      #       --model models/Qwen2.5-0.5B-Instruct/ggml-model-f16.gguf \
      #         -c 8192 \
      #         -b 512 \
      #         -ub 256 \
      #         -t 16 \
      #         -npp 128,256 \
      #         -ntg 128,256 \
      #         -npl 1,2,4,8,16 \
      #         --output-format jsonl

      # - name: Run llama-parallel
      #   working-directory: llama.cpp
      #   run: |
      #     ./build/bin/llama-parallel \
      #       --model models/Qwen2.5-0.5B-Instruct/ggml-model-f16.gguf \
      #         -p "The quick brown fox jumps over the lazy dog." \
      #         -n 128 \
      #         -c 8192 \
      #         -b 512 \
      #         -ub 128 \
      #         -t 8 \
      #         --sequences 8 \
      #         --junk 4 \
      #         -pps \
      #         --verbose-prompt

      # - name: Run llama-mtmd-cli
      #   working-directory: llama.cpp
      #   env:
      #     GGML_CANN_DISABLE: "1"
      #   run: |
      #     ./build/bin/llama-mtmd-cli \
      #       --model models/Qwen2.5-VL-3B-Instruct/ggml-model-f16.gguf \
      #       --mmproj models/Qwen2.5-VL-3B-Instruct/mmproj.gguf \
      #       --image media/llama0-banner.png \
      #       -p "Describe the image in detail." \
      #       -t 4 \
      #       -n 128 \
      #       --chat-template vicuna

  create-pr:
      if: ${{ needs.test.outputs.check-changed == 'true' }}
      name: Update benchmark result
      runs-on: ubuntu-latest
      environment: Ascend
      needs: test
      steps:
        - name: Checkout
          uses: actions/checkout@v4
  
        - name: Download llamacpp_ops.md artifact
          uses: actions/download-artifact@v4
          with:
            name: llamacpp_ops.md
            path: docs/llamacpp_ops.md
  
        # See: https://github.com/peter-evans/create-pull-request
        - name: Create a pull request to update benchmark result
          uses: peter-evans/create-pull-request@v7
          with:
            token: ${{ secrets.GITHUB_TOKEN }}
            base: main
            branch: llamacpp-report/${{ github.run_id }}
            commit-message: "Update llamacpp backend ops support report"
            add-paths: docs/llamacpp_ops.md
            title: "[Ascend NPU] Update llamacpp backend ops support report"
            body: |
              This PR updates the llama.cpp backend operator support report based on the latest test results.
              The following are the details of the test run:
  
              - [Workflow run][1]
              - [ops report][2] (click to download)
  
              [1]: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
              [2]: ${{ needs.test.outputs.upload-report }}
  
              cc: xuedinge233
              # cc: @FFFrog @hipudding
            reviewers: xuedinge233
