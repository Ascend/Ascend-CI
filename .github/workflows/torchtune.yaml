name: "torchtune single device test"

on:
  workflow_dispatch:
      inputs:
        runner:
          required: true
          type: choice
          options:
            - self-hosted
            - linux-arm64-npu-1
            - linux-arm64-npu-2
            - linux-arm64-npu-4
          default: 'linux-arm64-npu-1'
          description: 'Runner to run on'
        image:
          required: true
          type: choice
          options:
            - ascendai/cann:7.0.1-910b-ubuntu22.04-py3.8
            - ascendai/cann:8.0.0-910b-ubuntu22.04-py3.10
            - ascendai/cann:latest
          default: "ascendai/cann:latest"
          description: "The docker image which will be loaded"
        device:
          required: true
          type: choice
          options:
            - /dev/davinci0
            - /dev/davinci1
            - /dev/davinci2
            - /dev/davinci3
            - /dev/davinci4
            - /dev/davinci5
            - /dev/davinci6
            - /dev/davinci7
          default: '/dev/davinci0'
          description: 'Device to use'

  pull_request:
    branches: 
      - 'torchtune'
    paths:
      - '.github/workflows/torchtune.yaml'
  push:
    branches: 
      - 'torchtune'
    paths:
      - '.github/workflows/torchtune.yaml'

defaults:
  run:
    shell: bash -el {0}

jobs:
  prepare:
    runs-on: ubuntu-latest
    outputs:
      runner: ${{ steps.set.outputs.runner }}
      image: ${{ steps.set.outputs.image }}
      device: ${{ steps.set.outputs.device }}
    steps:
      - id: set
        run: |
          echo "runner=${{ github.event.inputs.runner || 'linux-arm64-npu-1' }}" >> $GITHUB_OUTPUT
          echo "image=${{ github.event.inputs.image || 'ascendai/cann:latest' }}" >> $GITHUB_OUTPUT
          echo "device=${{ github.event.inputs.device || '/dev/davinci0' }}" >> $GITHUB_OUTPUT
          
  torchtune:
    needs: prepare
    name: run torchtune for torch_npu
    
    runs-on: ${{ needs.prepare.outputs.runner }}
    container:
      image: ${{ needs.prepare.outputs.image }}
      env:
        HF_ENDPOINT: https://hf-mirror.com
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
        
    steps:
      - name: Show NPU info
        run: |
          npu-smi info

      - name: Config mirrors
        run: |
          sed -i 's|ports.ubuntu.com|mirrors.tuna.tsinghua.edu.cn|g' /etc/apt/sources.list
          pip config set global.index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple

      - name: Install system dependencies
        run: |
          apt-get update
          apt-get install -y \
              git gcc g++ make cmake ninja-build curl \
              libgl1 libglib2.0-0 libsndfile1

      - name: Checkout torchtune
        uses: actions/checkout@v4
        with:
          repository: pytorch/torchtune
          path: torchtune

      - name: Install torch_npu
        run: |
          pip install torch torch_npu==2.6.0rc1 torchvision torchao --extra-index-url https://download.pytorch.org/whl/nightly/cpu
          
      - name: Install torchtune
        working-directory: torchtune
        run: |
          pip install torchtune
            
      - name: Show environment info
        run: |
          pip list

      - name: Download Qwen2.5 model
        run: |
          export HF_ENDPOINT=https://hf-mirror.com
          huggingface-cli download --resume-download Qwen/Qwen2.5-0.5B-Instruct \
            --local-dir /tmp/Qwen2.5-0.5B-Instruct # \
            # --token os.environ["HF_TOKEN"]

      - name: Run torchtune with lora finetune
        run: |
          tune run lora_finetune_single_device --config qwen2_5/0.5B_lora_single_device || true
      
      - name: Run torchtune with full finetune
        run: |
          tune run full_finetune_single_device --config qwen2_5/0.5B_full_single_device || true
