name: Test torchtitan

on:
  workflow_dispatch:
    inputs:
      runner:
        required: true
        type: choice
        options:
          - linux-arm64-npu-1
          - linux-arm64-npu-2
          - linux-arm64-npu-4
          - linux-aarch64-310p-1
          - linux-aarch64-310p-2
        default: "linux-arm64-npu-1"
        description: "The runner selected to run on"
    
  pull_request:
    branches:
      - "torchtitan"
    paths:
      - ".github/workflows/torchtitan.yml"
  push:
    branches:
      - "torchtitan"
    paths:
      - ".github/workflows/torchtitan.yml"

# Bash shells do not use ~/.profile or ~/.bashrc so these shells need to be explicitly
# declared as "shell: bash -el {0}" on steps that need to be properly activated.
# It's used to activate ascend-toolkit environment variables.

defaults:
  run:
    shell: bash -el {0}

jobs:
  setup_environment:
    name: run torchtitan tests
    runs-on: ${{ inputs.runner || 'linux-arm64-npu-4' }}
    container: 
      image: swr.cn-southwest-2.myhuaweicloud.com/base_image/ascend-ci/cann:8.2.rc1-910b-ubuntu22.04-py3.11
      env:
        HF_ENDPOINT: https://hf-mirror.com
    steps:
      - name: Show NPU info
        run: |
          npu-smi info

      - name: Config mirrors
        run: |
          sed -i 's|ports.ubuntu.com|mirrors.tuna.tsinghua.edu.cn|g' /etc/apt/sources.list
          pip config set global.index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple

      - name: Install system dependencies
        run: |
          apt-get update
          apt-get install -y \
              git gcc g++ make cmake ninja-build curl \
              libgl1 libglib2.0-0 libsndfile1

      - name: Config git
        run: |
          git config --global --add safe.directory "$GITHUB_WORKSPACE"
          git config --global url."https://gh-proxy.test.osinfra.cn/https://github.com/".insteadOf https://github.com/

      - name: Checkout
        uses: actions/checkout@v4

      - name: Checkout benchmark
        uses: actions/checkout@v4
        with:
          repository: pytorch/torchtitan
          path: torchtitan

      - name: Install torch_npu dependencies
        run: |
          pip install torch torch_npu==2.9.0rc1 torchdata torchao torchvision torchaudio triton

      - name: Install torchtitan
        working-directory: torchtitan
        run: |
          pip install -r requirements-dev.txt
          pip install -r requirements.txt
          # pip install torchtitan

      - name: Install project dependencies
        run: |
          pip install pytest pytest-cov tyro tabulate tokenizers transformers datasets pyyaml setuptools auditwheel tensorboard

      - name: Show environment info
        id: check_npu
        run: |
          npu_is_available=$(python -c "import torch; print(torch.npu.is_available())")
          npu_count=$(python -c "import torch; print(torch.npu.device_count())")
          echo "npu_count=${npu_count}" >> $GITHUB_OUTPUT
          echo "NPU is available: ${npu_is_available}"
          echo "NPU count: ${npu_count}"
          pip list | grep -E 'torch|numpy'

      - name: Patch torch.compile + stub torch.nn.attention.varlen (CI workaround)
        working-directory: torchtitan
        run: |
          cat > sitecustomize.py << 'EOF'
          import sys, types, re
      
          # --- 1) Stub torch.nn.attention.varlen so torchtitan import doesn't crash ---
          try:
              import torch.nn.attention as _attn_mod
          except Exception:
              _attn_mod = None
      
          mod_name = "torch.nn.attention.varlen"
          if mod_name not in sys.modules:
              m = types.ModuleType(mod_name)
              def varlen_attn(*args, **kwargs):
                  raise RuntimeError(
                      "torch.nn.attention.varlen is not available in this build; "
                      "stub injected for CI."
                  )
              m.varlen_attn = varlen_attn
              sys.modules[mod_name] = m
              if _attn_mod is not None:
                  setattr(_attn_mod, "varlen", m)
      
          # --- 2) Make torch.compile tolerant to unsupported inductor options ---
          try:
              import torch
              _orig_compile = torch.compile
      
              def _safe_compile(*args, **kwargs):
                  try:
                      return _orig_compile(*args, **kwargs)
                  except RuntimeError as e:
                      msg = str(e)
                      # Handle: Unexpected optimization option <foo>, known options are [...]
                      if "Unexpected optimization option" in msg:
                          # Option A: drop all options and retry
                          kwargs2 = dict(kwargs)
                          kwargs2.pop("options", None)
                          return _orig_compile(*args, **kwargs2)
                      raise
      
              torch.compile = _safe_compile
          except Exception:
              # If torch itself can't import, do nothing
              pass
          EOF    
 
      - name: ignore init_process_group _ranks
        run: |
          cat > sitecustomize.py <<'EOF'
          # Loaded automatically by Python at startup if present on sys.path.
          def _patch_init_process_group():
              try:
                  import torch.distributed as dist
              except Exception:
                  return
      
              orig = dist.init_process_group
      
              def wrapped(*args, **kwargs):
                  try:
                      return orig(*args, **kwargs)
                  except TypeError as e:
                      msg = str(e)
                      # torchtitan passes internal kwarg _ranks; many builds (incl. torch_npu) don't support it
                      if "_ranks" in kwargs and "unexpected keyword argument" in msg:
                          kwargs2 = dict(kwargs)
                          kwargs2.pop("_ranks", None)
                          return orig(*args, **kwargs2)
                      raise
      
              dist.init_process_group = wrapped
      
              # Some code paths import from torch.distributed.distributed_c10d
              try:
                  import torch.distributed.distributed_c10d as dc10d
                  dc10d.init_process_group = wrapped
              except Exception:
                  pass
      
          _patch_init_process_group()
          EOF
        

      - name: Run torchtitan integration_test
        working-directory: torchtitan
        run: |
          mkdir artifacts
          python -m tests.integration_tests.run_tests artifacts \
          --gpu_arch_type rocm \
          --test_suite features \
          --ngpu ${{ steps.check_npu.outputs.npu_count }} \
          || true

      - name: Run torchtitan unittest
        working-directory: torchtitan
        run: |
          pytest tests/unit_tests --cov=. --cov-report=xml --durations=20 -vv
